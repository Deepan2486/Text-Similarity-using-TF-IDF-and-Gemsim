{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "89124ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6c1170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_docs = []\n",
    "\n",
    "with open ('demoDoc.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file_docs.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b12e049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mars is the fourth planet in our solar system.',\n",
       " 'It is second-smallest planet in the Solar System after Mercury.',\n",
       " 'Saturn is yellow planet.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "683d4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in file_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64f1d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bfff55b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mars',\n",
       "  'is',\n",
       "  'the',\n",
       "  'fourth',\n",
       "  'planet',\n",
       "  'in',\n",
       "  'our',\n",
       "  'solar',\n",
       "  'system',\n",
       "  '.'],\n",
       " ['it',\n",
       "  'is',\n",
       "  'second-smallest',\n",
       "  'planet',\n",
       "  'in',\n",
       "  'the',\n",
       "  'solar',\n",
       "  'system',\n",
       "  'after',\n",
       "  'mercury',\n",
       "  '.'],\n",
       " ['saturn', 'is', 'yellow', 'planet', '.']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de92c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e9d1fdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'fourth': 1,\n",
       " 'in': 2,\n",
       " 'is': 3,\n",
       " 'mars': 4,\n",
       " 'our': 5,\n",
       " 'planet': 6,\n",
       " 'solar': 7,\n",
       " 'system': 8,\n",
       " 'the': 9,\n",
       " 'after': 10,\n",
       " 'it': 11,\n",
       " 'mercury': 12,\n",
       " 'second-smallest': 13,\n",
       " 'saturn': 14,\n",
       " 'yellow': 15}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "92b21483",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs] #since there are multiple tokenized sentences in gen_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "286ae2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1)],\n",
       " [(0, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1)],\n",
       " [(0, 1), (3, 1), (6, 1), (14, 1), (15, 1)]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8963b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea8dce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fourth', 0.53], ['in', 0.2], ['mars', 0.53], ['our', 0.53], ['solar', 0.2], ['system', 0.2], ['the', 0.2]]\n",
      "[['in', 0.17], ['solar', 0.17], ['system', 0.17], ['the', 0.17], ['after', 0.47], ['it', 0.47], ['mercury', 0.47], ['second-smallest', 0.47]]\n",
      "[['saturn', 0.71], ['yellow', 0.71]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "991f8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('workdir/',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad464fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2_docs = []\n",
    "\n",
    "with open ('queryDoc.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file2_docs.append(line)\n",
    "\n",
    "\n",
    "query_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in file2_docs]\n",
    "query_doc_bow = [dictionary.doc2bow(query_doc) for query_doc in query_docs]\n",
    "#query_doc_bow is the corpus of the query document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5fb739c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Saturn is the sixth planet from the Sun.', 'Mars is a red planet.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file2_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c3c388ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['saturn', 'is', 'the', 'sixth', 'planet', 'from', 'the', 'sun', '.'],\n",
       " ['mars', 'is', 'a', 'red', 'planet', '.']]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e1cfc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (3, 1), (6, 1), (9, 2), (14, 1)], [(0, 1), (3, 1), (4, 1), (6, 1)]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_doc_bow\n",
    "#query_doc_bow is the corpus of the query document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd55a34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (3, 1), (6, 1), (9, 2), (14, 1)]\n",
      "Comparing Result: [0.11641413 0.10281226 0.56890744]\n",
      "[(0, 1), (3, 1), (4, 1), (6, 1)]\n",
      "Comparing Result: [0.5311302 0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "for line in query_doc_bow:\n",
    "    query_doc_tf_idf = tf_idf[line]\n",
    "    print(line)\n",
    "    print('Comparing Result:', sims[query_doc_tf_idf]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "77319b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THEREFORE it can be seen that the last document sentence is the most similar to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "93efb3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_docs = []\n",
    "\n",
    "with open ('demoDoc2.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file_docs.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "03b9ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in file_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fd77c5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['malls',\n",
       "  'are',\n",
       "  'great',\n",
       "  'places',\n",
       "  'to',\n",
       "  'shop',\n",
       "  ',',\n",
       "  'i',\n",
       "  'can',\n",
       "  'find',\n",
       "  'sandwiches',\n",
       "  'under',\n",
       "  'one',\n",
       "  'roof',\n",
       "  '.'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'eating',\n",
       "  'toasted',\n",
       "  'cheese',\n",
       "  'and',\n",
       "  'tuna',\n",
       "  'sandwiches',\n",
       "  '.'],\n",
       " ['should',\n",
       "  'we',\n",
       "  'start',\n",
       "  'class',\n",
       "  'now',\n",
       "  ',',\n",
       "  'or',\n",
       "  'should',\n",
       "  'we',\n",
       "  'wait',\n",
       "  'for',\n",
       "  'everyone',\n",
       "  'to',\n",
       "  'get',\n",
       "  'here',\n",
       "  '?']]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7423c45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6b788714",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs] #since there are multiple tokenized sentences in gen_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2282744b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1)],\n",
       " [(1, 1),\n",
       "  (6, 1),\n",
       "  (11, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1)],\n",
       " [(0, 1),\n",
       "  (13, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 2),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 2)]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "80dd9e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[',', 0.11], ['.', 0.11], ['are', 0.31], ['can', 0.31], ['find', 0.31], ['great', 0.31], ['i', 0.11], ['malls', 0.31], ['one', 0.31], ['places', 0.31], ['roof', 0.31], ['sandwiches', 0.11], ['shop', 0.31], ['to', 0.11], ['under', 0.31]]\n",
      "[['.', 0.15], ['i', 0.15], ['sandwiches', 0.15], ['and', 0.4], ['cheese', 0.4], ['eating', 0.4], ['love', 0.4], ['toasted', 0.4], ['tuna', 0.4]]\n",
      "[[',', 0.09], ['to', 0.09], ['?', 0.23], ['class', 0.23], ['everyone', 0.23], ['for', 0.23], ['get', 0.23], ['here', 0.23], ['now', 0.23], ['or', 0.23], ['should', 0.47], ['start', 0.23], ['wait', 0.23], ['we', 0.47]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9b11c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('workdir/',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1f96a5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Result: [0.3690919  0.03038312 0.13209888]\n",
      "avg: 0.17719129721323648\n",
      "Comparing Result: [0.02851599 0.03681399 0.32011765]\n",
      "avg: 0.12848254044850668\n",
      "Comparing Result: [0.03263272 0.         0.9407785 ]\n",
      "avg: 0.3244704008102417\n",
      "Total similarity percentage: 63 %\n"
     ]
    }
   ],
   "source": [
    "avg_sims = [] # array of averages\n",
    "\n",
    "file2_docs = []\n",
    "\n",
    "with open ('queryDoc2.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file2_docs.append(line)\n",
    "# for line in query documents\n",
    "for line in file2_docs:\n",
    "        # tokenize words\n",
    "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "        # create bag of words\n",
    "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "        # find similarity for each document\n",
    "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "        # print (document_number, document_similarity)\n",
    "        print('Comparing Result:', sims[query_doc_tf_idf]) \n",
    "        # calculate sum of similarities for each query doc\n",
    "        sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "        # calculate average of similarity for each query doc\n",
    "        avg = sum_of_sims / len(file_docs)\n",
    "        # print average of similarity for each query doc\n",
    "        print(f'avg: {sum_of_sims / len(file_docs)}')\n",
    "        # add average values into array\n",
    "        avg_sims.append(avg)  \n",
    "   # calculate total average\n",
    "total_avg = np.sum(avg_sims, dtype=np.float)\n",
    "    # round the value and multiply by 100 to format it as percentage\n",
    "percentage_of_similarity = round(float(total_avg) * 100)\n",
    "    # if percentage is greater than 100\n",
    "    # that means documents are almost same\n",
    "if percentage_of_similarity >= 100:\n",
    "    percentage_of_similarity = 100\n",
    "    \n",
    "print(\"Total similarity percentage:\", percentage_of_similarity ,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4718b120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Malls are good for shopping.',\n",
       " 'What kind of bread is used for sandwiches?',\n",
       " 'Do we have to start class now, or should we wait for everyone to come here?']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file2_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "024abe20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17719129721323648, 0.12848254044850668, 0.3244704008102417]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7859cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
